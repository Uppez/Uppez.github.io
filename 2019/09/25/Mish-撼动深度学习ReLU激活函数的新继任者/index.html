<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <title>Mish:撼动深度学习ReLU激活函数的新继任者 | 教书的先生</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="对激活函数的研究一直没有停止过，ReLU还是统治着深度学习的激活函数，不过，这种情况有可能会被Mish改变。">
<meta name="keywords" content="深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Mish:撼动深度学习ReLU激活函数的新继任者">
<meta property="og:url" content="http://yoursite.com/2019/09/25/Mish-撼动深度学习ReLU激活函数的新继任者/index.html">
<meta property="og:site_name" content="教书的先生">
<meta property="og:description" content="对激活函数的研究一直没有停止过，ReLU还是统治着深度学习的激活函数，不过，这种情况有可能会被Mish改变。">
<meta property="og:locale" content="ch">
<meta property="og:image" content="https://i.loli.net/2019/09/25/PZMCh1OpjkRvYqf.jpg">
<meta property="og:image" content="https://i.loli.net/2019/09/25/Ya4lGfdXjN3o2qH.jpg">
<meta property="og:image" content="https://i.loli.net/2019/09/25/zrD2FSpymHT13s5.jpg">
<meta property="og:image" content="https://i.loli.net/2019/09/25/3udy29EoMmq8KwD.jpg">
<meta property="og:updated_time" content="2019-09-25T11:10:41.493Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mish:撼动深度学习ReLU激活函数的新继任者">
<meta name="twitter:description" content="对激活函数的研究一直没有停止过，ReLU还是统治着深度学习的激活函数，不过，这种情况有可能会被Mish改变。">
<meta name="twitter:image" content="https://i.loli.net/2019/09/25/PZMCh1OpjkRvYqf.jpg">
  
  <link rel="stylesheet" href="//cdn.bootcss.com/highlight.js/9.2.0/styles/github.min.css">
  <script src="//cdn.bootcss.com/highlight.js/9.2.0/highlight.min.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', (event) => {
      document.querySelectorAll('pre code').forEach((block) => {
        hljs.highlightBlock(block);
      });
    });
  </script>
  <link rel="stylesheet" href="/css/index.css">
</head>
</html>
<body style="


  background-color: #eff0f6

">
  <div id="container">
    <nav id="nav">
  <header class="header">
    <a href="/" class="title">Clover Tuan</a>
  </header>
  <div class="ctnWrap">
    <div class="icons">
      
        
          
            <a href="https://dribbble.com/clovertuan" target="_blank" class="nav-icn iconfont icon-dribbble"></a>
          
        
          
            <a href="https://www.behance.net/clovertuan" target="_blank" class="nav-icn iconfont icon-behance"></a>
          
        
          
            <a href="http://clovertuan.lofter.com/" target="_blank" class="nav-icn iconfont icon-lofter"></a>
          
        
          
            <a href="https://www.instagram.com/clovertuan/" target="_blank" class="nav-icn iconfont icon-instagram"></a>
          
        
          
            <a href="https://github.com/cloverTuan" target="_blank" class="nav-icn iconfont icon-github"></a>
          
        
      
    </div>
    <div class="menu">
      
        
            <a href="/" class="nav-menu ">HOME</a>
          
        
            <a href="/archives" class="nav-menu ">ARCHIVE</a>
          
        
            <a href="/about" class="nav-menu ">ABOUT</a>
          
        
      
    </div>
  </div>
</nav>
    <div id="main"><section class="article">
  <h2 class="title">Mish:撼动深度学习ReLU激活函数的新继任者</h2>
  <p class="sub">Sep 25, 2019</p>
  <article class="content">
    <p>Diganta Misra的一篇题为“Mish: A Self Regularized Non-Monotonic Neural Activation Function”的新论文介绍了一个新的深度学习激活函数，该函数在最终准确度上比Swish(+.494%)和ReLU(+ 1.671%)都有提高。</p>
<p>他们的小型FastAI团队使用Mish代替ReLU，打破了之前在FastAI全球排行榜上准确性得分记录的一部分。结合Ranger优化器，Mish激活，Flat + Cosine 退火和自注意力层，他们能够获得12个新的排行榜记录！</p>
<p><a href="https://sm.ms/image/PZMCh1OpjkRvYqf" target="_blank"><img src="https://i.loli.net/2019/09/25/PZMCh1OpjkRvYqf.jpg"></a></p>
<p>我们12项排行榜记录中的6项。每条记录都是用Mish而不是ReLU。(蓝色高亮显示，400 epoch的准确率为94.6，略高于我们的20 epoch的准确率为93.8:)</p>
<p>作为他们自己测试的一部分，对于ImageWoof数据集的5 epoch测试，他们说：</p>
<blockquote>
<p>Mish在高显著性水平上优于ReLU (P &lt; 0.0001)。(FastAI论坛@ Seb)</p>
</blockquote>
<p>Mish已经在70多个基准上进行了测试，包括图像分类、分割和生成，并与其他15个激活函数进行了比较。</p>
<h1 id="什么是Mesh"><a href="#什么是Mesh" class="headerlink" title="什么是Mesh"></a>什么是Mesh</h1><p>直接看Mesh的代码会更简单一点，简单总结一下，Mish=x * tanh(ln(1+e^x))。</p>
<p>其他的激活函数，ReLU是x = max(0,x)，Swish是x * sigmoid(x)。</p>
<p><strong>PyTorch的Mish实现</strong>：</p>
<p><a href="https://sm.ms/image/Ya4lGfdXjN3o2qH" target="_blank"><img src="https://i.loli.net/2019/09/25/Ya4lGfdXjN3o2qH.jpg"></a></p>
<p><strong>Tensorflow中的Mish函数</strong>：</p>
<p>Tensorflow：x = x *tf.math.tanh(F.softplus(x))</p>
<h1 id="Mish和其他的激活函数相比怎么样？"><a href="#Mish和其他的激活函数相比怎么样？" class="headerlink" title="Mish和其他的激活函数相比怎么样？"></a>Mish和其他的激活函数相比怎么样？</h1><p>下图显示了Mish与其他一些激活函数的测试结果。这是多达73个测试的结果，在不同的架构，不同的任务上：</p>
<p><a href="https://sm.ms/image/zrD2FSpymHT13s5" target="_blank"><img src="https://i.loli.net/2019/09/25/zrD2FSpymHT13s5.jpg"></a></p>
<h1 id="为什么Mish表现这么好？"><a href="#为什么Mish表现这么好？" class="headerlink" title="为什么Mish表现这么好？"></a>为什么Mish表现这么好？</h1><p>以上无边界(即正值可以达到任何高度)避免了由于封顶而导致的饱和。理论上对负值的轻微允许允许更好的梯度流，而不是像ReLU中那样的硬零边界。</p>
<p>最后，可能也是最重要的，目前的想法是，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化。</p>
<p>尽管如此，我测试了许多激活函数，它们也满足了其中的许多想法，但大多数都无法执行。这里的主要区别可能是Mish函数在曲线上几乎所有点上的平滑度。</p>
<p>这种通过Mish激活曲线平滑性来推送信息的能力如下图所示，在本文的一个简单测试中，越来越多的层被添加到一个测试神经网络中，而没有一个统一的函数。随着层深的增加，ReLU精度迅速下降，其次是Swish。相比之下，Mish能更好地保持准确性，这可能是因为它能更好地传播信息：</p>
<p><a href="https://sm.ms/image/3udy29EoMmq8KwD" target="_blank"><img src="https://i.loli.net/2019/09/25/3udy29EoMmq8KwD.jpg"></a></p>
<p>更平滑的激活功能允许信息更深入地流动……注意，随着层数的增加，ReLU快速下降。</p>
<h1 id="如何把Mish放到你自己的网络中？"><a href="#如何把Mish放到你自己的网络中？" class="headerlink" title="如何把Mish放到你自己的网络中？"></a>如何把Mish放到你自己的网络中？</h1><p>Mish的PyTorch和FastAI的源代码可以在github的两个地方找到：</p>
<p>1、官方Mish github：<a href="https://github.com/digantamisra98/Mish" target="_blank" rel="noopener">https://github.com/digantamisra98/Mish</a></p>
<p>2、非官方的Mish使用inline提升速度：<a href="https://github.com/lessw2020/mish" target="_blank" rel="noopener">https://github.com/lessw2020/mish</a></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>ReLU有一些已知的弱点，但是通常它执行起来很轻，并且在计算上很轻。Mish具有较强的理论渊源，在测试中，就训练稳定性和准确性而言，Mish的平均性能优于ReLU。</p>
<p>复杂度只稍微增加了一点(V100 GPU和Mish，相对于ReLU，每epoch增加大约1秒)，考虑到训练稳定性的提高和最终精度的提高，稍微增加一点时间似乎是值得的。</p>
<p>最终，在今年测试了大量新的激活函数后，Mish在这方面处于领先地位，许多人怀疑它很有可能成为AI未来的新ReLU。</p>
<p>英文文章地址：<a href="https://medium.com/@lessw/meet-mish-new-state-of-the-art-ai-activation-function-the-successor-to-relu-846a6d93471f" target="_blank" rel="noopener">https://medium.com/@lessw/meet-mish-new-state-of-the-art-ai-activation-function-the-successor-to-relu-846a6d93471f</a></p>
  </article>
  <footer class="f-cf">
    
      <a href="/2019/09/25/南大周志华关于论文内容演讲演示文稿/" class="link f-fl">⟵南大周志华关于论文内容演讲演示文稿</a>
    
    
      <a href="/2019/09/25/算法可视化平台/" class="link f-fr">算法可视化平台⟶</a>
    
  </footer>
</section></div>
    <footer id="footer" class="f-cf">
  d.guangying@foxmail.com
  
    
      
        · <a href="https://dribbble.com/clovertuan" target="_blank" class="nav-icn">Dribbble</a>
      
    
      
        · <a href="https://www.behance.net/clovertuan" target="_blank" class="nav-icn">Behance</a>
      
    
      
        · <a href="http://clovertuan.lofter.com/" target="_blank" class="nav-icn">Lofter</a>
      
    
      
        · <a href="https://www.instagram.com/clovertuan/" target="_blank" class="nav-icn">Instagram</a>
      
    
      
        · <a href="https://github.com/cloverTuan" target="_blank" class="nav-icn">GitHub</a>
      
    
  
  <span class="copyright">All rights reserved @Clover Tuan</span>
</footer>
  </div>
</body>
</html>